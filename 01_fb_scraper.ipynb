{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec03ebd1-6ffc-486f-aaa0-572d800aa318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp fb_scraper "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46593b57-d6d9-4373-9629-324ba16e9fdd",
   "metadata": {},
   "source": [
    "# fb_scraper\n",
    "> Module for scraping Facebook posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc78a86-22a8-48f3-b34e-36e39c3e39d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056a4e23-9060-453b-8431-5384d410f935",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import progressbar\n",
    "from pprint import pprint\n",
    "from random import randint\n",
    "from time import sleep\n",
    "from facebook_scraper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca78cdb-a098-44b7-84cf-a768df6372d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def filter_post(post:dict, keywords:list=None) -> pd.DataFrame:\n",
    "    '''\n",
    "    Function to filter a post `dict` for keywords\n",
    "    '''\n",
    "    post_keys = ['post_id', 'text', 'post_text', 'time', 'timestamp', 'likes', 'comments','shares',\n",
    "                 'post_url', 'user_id', 'username','user_url', 'reactions',\n",
    "                 'reaction_count']\n",
    "    \n",
    "    if keywords:\n",
    "        post_keys = keywords\n",
    "    try:\n",
    "        post_dict = {key: post[key] for key in post_keys}\n",
    "    except Exception as e:\n",
    "        print(\"Key not found in post dict\")\n",
    "        raise e\n",
    "        \n",
    "    return pd.Dataframe(post_dict, index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ca5bb9-722f-476a-ba9a-ea6d867a5f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"filter_post\" class=\"doc_header\"><code>filter_post</code><a href=\"__main__.py#L3\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>filter_post</code>(**`post`**:`dict`, **`keywords`**:`list`=*`None`*)\n",
       "\n",
       "Function to filter a post `dict` for keywords"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(filter_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239986d7-654c-4224-8e76-d71209be16b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "#add tests for filter posts\n",
    "dummy_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5169f22-078d-415d-acbc-a426185858e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def extract_comments(comments:list, meta:dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to extract comments from Facebook posts\n",
    "    \n",
    "    Args:\n",
    "        comments: list of dictionaries. Each comment is a dictionary\n",
    "        meta: dictionary that contains meta information from the post\n",
    "    \"\"\"\n",
    "    comment_keys = ['comment_id', 'comment_url', 'comment_text', 'comment_time',\n",
    "                    'comment_reactions', 'comment_reaction_count', 'replies']\n",
    "    comments_dict = [{key: comment[key] for key in comment_keys} for comment in comments]\n",
    "    df = pd.DataFrame(comments_dict)\n",
    "    for key, val in meta.items():\n",
    "        df[key] = val\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bf5336-f08b-44ad-a38e-d5e3ee861b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"extract_comments\" class=\"doc_header\"><code>extract_comments</code><a href=\"__main__.py#L3\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>extract_comments</code>(**`comments`**:`list`, **`meta`**:`dict`)\n",
       "\n",
       "Function to extract comments from Facebook posts\n",
       "\n",
       "Args:\n",
       "    comments: list of dictionaries. Each comment is a dictionary\n",
       "    meta: dictionary that contains meta information from the post"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(extract_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b4d24c-e49c-4294-a66f-33e00b019cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "#add tests for extract_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421bc9c1-5503-455b-b097-367b344a535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FbScraper:\n",
    "    \"\"\"\n",
    "    A scraper object that runs the scraping process and stores posts and comments\n",
    "    \"\"\"\n",
    "    def __init__(self, site:str, stop_date:str, num_pages:int=100, timeout:set=(60,120)):\n",
    "        self.cookie = None\n",
    "        self.posts = []\n",
    "        self.comments = []\n",
    "        \n",
    "    def set_cookie(self, cookie:dict):\n",
    "        \"Set a cookie to use for scraping FB. Usually grab from Dataiku globals\"\n",
    "        self.cookie = cookie\n",
    "        \n",
    "    def scrape(self):\n",
    "        \"\"\"\n",
    "        start scraping using the current configuration\n",
    "        \"\"\"\n",
    "        post_ids = list()\n",
    "        saved_posts = []\n",
    "        for i, row in progressbar.progressbar(list(df_sites.iterrows())):\n",
    "        \n",
    "            start_url = None\n",
    "            count = 0\n",
    "\n",
    "            while True:\n",
    "                    for post in get_posts(row['id'], pages=GET_PAGES, start_url=start_url,\n",
    "                                          options={\"comments\": True, \"sleep\": 0,\n",
    "                                                   \"posts_per_page\": 1, \"days-limit\": 90},\n",
    "                                         cookies=cookie):\n",
    "\n",
    "\n",
    "\n",
    "                        saved_posts.append(post)\n",
    "                        print(post['text'])\n",
    "\n",
    "                        if post['post_id'] in post_ids:\n",
    "                            if interrupt>0:\n",
    "                                interrupt = 0\n",
    "                                break\n",
    "                            else:\n",
    "                                interrupt +=1\n",
    "                                continue\n",
    "\n",
    "                        try:\n",
    "\n",
    "                            meta = {'post_id':post['post_id'], 'username': post['username']}\n",
    "                            comments = extract_comments(post['comments_full'], meta)\n",
    "                            post_filtered = filter_post(post)\n",
    "\n",
    "                            fb_comments = dataiku.Dataset(\"fb_comments\")\n",
    "                            fb_comments.write_with_schema(comments)\n",
    "\n",
    "\n",
    "                            fb_posts = dataiku.Dataset(\"fb_posts\")\n",
    "                            fb_posts.write_with_schema(post_filtered)\n",
    "\n",
    "                            post_ids.append(post['post_id'])\n",
    "\n",
    "                            print(\"-\"*30)\n",
    "                            print(f'Scraped POST: {post[\"text\"][:25]}')\n",
    "                            print(f'LAST DATE: {post[\"time\"]}')\n",
    "                            print(f'TOTAL: {len(post_ids)} posts fetched')\n",
    "                            print(\"-\"*30)\n",
    "\n",
    "                            sleep(randint(60,120))\n",
    "\n",
    "\n",
    "                            if post['time'].year==2020:\n",
    "\n",
    "                                break\n",
    "\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            sleep(randint(60,120))\n",
    "                            continue\n",
    "                    print(f'all posts from {row[\"id\"]} scraped')\n",
    "                    sleep(randint(60,120))\n",
    "                    break\n",
    "                    \n",
    "        def get_posts():\n",
    "            return pd.concat(self.posts)\n",
    "        \n",
    "        def get_comments():\n",
    "            return pd.concat(self.comments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc9217b-be0b-425c-8e6d-3b5ab9a5f64e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"FbScraper.__init__\" class=\"doc_header\"><code>FbScraper.__init__</code><a href=\"__main__.py#L6\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>FbScraper.__init__</code>(**`site`**:`str`, **`stop_date`**:`str`, **`num_pages`**:`int`=*`100`*, **`timeout`**:`set`=*`(60, 120)`*)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(FbScraper.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd60e68-cd57-49cf-85b5-5d249d2cb5f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"FbScraper.set_cookie\" class=\"doc_header\"><code>FbScraper.set_cookie</code><a href=\"__main__.py#L11\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>FbScraper.set_cookie</code>(**`cookie`**:`dict`)\n",
       "\n",
       "Set a cookie to use for scraping FB. Usually grab from Dataiku globals"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(FbScraper.set_cookie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0469869a-54a5-4828-957f-1e96a10d68da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"FbScraper.scrape\" class=\"doc_header\"><code>FbScraper.scrape</code><a href=\"__main__.py#L15\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>FbScraper.scrape</code>()\n",
       "\n",
       "start scraping using the current configuration"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(FbScraper.scrape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
